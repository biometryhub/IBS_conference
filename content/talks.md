+++
title = "Presentation Titles, Abstracts and Slides"
description = ""
+++



<ul class="nav nav-tabs">
  <li class="active"><a data-toggle="tab" href="#tuesday">Tue</a></li>
  <li><a data-toggle="tab" href="#wednesday">Wed</a></li>
  <li><a data-toggle="tab" href="#thursday">Thu</a></li>
  <li><a data-toggle="tab" href="#friday">Fri</a></li>
</ul>
<div class="tab-content">
  <div id="tuesday" class="tab-pane fade in active">
    <h2>Invited Speakers: Medical</h2>
    <h4><i>Room: Exhibition Hall</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse1" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive">
      <table class="abstract-table">
        <thead>
          <tr>
            <th>Presenter</th>
            <th>Abstract Title</th>
            <th>Time</th>
            <th>Slides</th>
          </tr>
        </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="9" data-target=".9collapsed">
            <td>James Carpenter</td>
            <td>Multilevel multiple imputation for health and survey data: your flexible (and robust) friend
            </td>
            <td>9:30 - 10:30</td>
            <td></td>
          </tr>
          <tr class="out budgets 9collapsed collapse multi-collapse1" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Multiple imputation is now well established as a practical and flexible method for analyzing partially observed data under the missing at random assumption. However, in large datasets there are concerns about how to preserve heterogeneity in the relationship between variables in the imputation process.<p>Building on recent work, we describe an imputation model (and R software) which allows the covariance matrix of the variables to vary randomly across higher level units, which may represent health districts or hospitals.<p>We further show how this approach allows us to (i) include weights, when the substantive model is weighted; (ii) provide a degree of robustness to misspecification of the imputation model and (iii) extends to impute data consistent with interaction and non-linear effects under investigation.<p>We illustrate with examples from the UK Millennium Cohort Study and the UK Clinical Practice Research Datalink.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="10" data-target=".10collapsed">
            <td>Max Moldovan</td>
            <td>Pursuing the cancer-schizophrenia disassociation paradox: genomes, phenomes and intimate conversatio</td>
            <td>13:30 - 14:30</td>
            <td></td>
          </tr>
          <tr class="out budgets 10collapsed collapse multi-collapse1" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Positive and negative empirical findings of schizophrenia being protective against cancer remain a controversy and an active topic for debates among the intersection of oncologists, psychiatrists, epidemiologists and a diverse group of research scientists interested in the topic. I will present the vision of the paradox from different points of view, using a number of analysis and visualisation approaches from my current data science toolbox.<p>Firstly, I will take a position of a "genes-rule-it-all" theory proponent (while, in fact, being an opponent of the paradigm). I will share my experience of the exposure to genomics, and how my hopes and excitement were cut short by the inability of a SNP model to predict a well-defined phenotype when taken out of sample. Staying in the same role, I will introduce the Molecular Signatures Database (MSigDB) and an attempt to look at this gene expression signature information resource from a different angle.<p>Secondly, I will review epidemiological explanations behind the cancer-schizophrenia disassociation paradox, presenting the pair of disorders within the whole phenome network. While biases can help to justify the paradox, the doubt remains when one looks at bare numbers.<p>Finally, I will introduce a "geocentrism versus heliocentrism in cancer research" hypothesis and speculate about the role of dogmas in science, rates of clinical translation and prospects of moving to the brighter future.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="11" data-target=".11collapsed">
            <td>Richard Cook</td>
            <td>Missing outcomes in stepped-wedge trials</td>
            <td>11:40 - 12:00</td>
            <td></td>
          </tr>
          <tr class="out budgets 11collapsed collapse multi-collapse1" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          TBA<br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
    <!--                -->
    <!----First Stream --->
    <!--                -->
    <h2>Contributed Talks Session 1a: Biostatistics I</h2>
    <h4><i>Room: The Gallery</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive tg-wrap">
      <table class="abstract-table">
        <thead>
          <tr>
            <th>Presenter</th>
            <th>Abstract Title</th>
            <th>Time</th>
            <th>Slides</th>
          </tr>
        </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="1" data-target=".1collapsed">
            <td>Hans Hockey</td>
            <td>Hockey sticks and broken sticks continued – enhancing the gold standard RCT for chronic diseases</td>
            <td>11:00 - 11:20</td>
            <td></td>
          </tr>
          <tr class="out budgets 1collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          This work was motivated by a previously given chronic rare disease real data example as analysed by a longitudinal hockey stick model. The gold standard randomized controlled trial (RCT) compares active and placebo treatment arms at a post-baseline timepoint sufficiently late enough for an active effect to be apparent. For this design the analysis of covariance of final values adjusted for baseline values is standard (hopefully!).<p>Similar but enhanced alternative design and analysis combinations in chronic diseases will be presented which have several possible advantages: there is the possibility of assessing the size of any placebo effect; there is less ethical and recruitment need to have the active group larger than the placebo group; missing data, particularly of the final value, is less critical to the analysis.<p>It is also conjectured that some of the circumstances which can favour these enhanced designs can also be used to help argue for single arm studies in rare chronic diseases, despite the lack of randomization.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="2" data-target=".2collapsed">
            <td>Jessica Kasza</td>
            <td>From the stepped wedge to the staircase: the information content of stepped wedge trials</td>
            <td>11:20 - 11:40</td>
            <td></td>
          </tr>
          <tr class="out budgets 2collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Stepped wedge cluster randomised trials are a type of longitudinal cluster randomised trial in which clusters, e.g. schools, hospitals, or geographical regions, are randomised to a particular set of treatment sequences. In standard stepped wedge trials, all clusters start in the control condition before switching, in a randomised order, to the intervention. The application of stepped wedge trials is increasing rapidly: a 2011 systematic review of published or registered stepped wedge trials found only 25, but as of July 2019, 210 stepped wedge trials were registered on clinicaltrials.gov. Stepped wedge trials are particularly useful in assessing interventions that will be rolled out or cannot be undone, e.g. changes in policies or cluster-wide education campaigns. However, stepped wedge trials are expensive and burdensome, requiring that all clusters contribute measurements for the entire trial duration. Recent work has shown that different cluster-period “cells” of the stepped wedge contribute different amounts of information to the estimation of the intervention effect. Such work suggests that “incomplete” stepped wedge trials in which clusters contribute measurements in a restricted set of trial periods may provide efficient alternatives to the full stepped wedge. <p>In this talk I will discuss the amount of information contributed by cluster-period cells of stepped wedge trials to the estimation of the effect of an intervention, where this is quantified by the increase in the variance of the intervention effect estimator when that cell is omitted. I will consider the impact of within-cluster correlation structure, treatment effect heterogeneity, implementation periods, and unequal cluster-period sizes on the pattern of information content. This work indicates that in many scenarios, “staircase” trials, a particular type of incomplete stepped wedge in which clusters provide measurements immediately before and after the treatment switch only, may prove to be efficient and less burdensome alternatives to the complete stepped wedge.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="3" data-target=".3collapsed">
            <td>Md Anower Hossain</td>
            <td>Missing outcomes in stepped-wedge trials</td>
            <td>11:40 - 12:00</td>
            <td></td>
          </tr>
          <tr class="out budgets 3collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          In stepped-wedge cluster randomised trials (SW-CRTs), clusters are sequentially randomised until the point at which all clusters are exposed to the intervention. Two commonly used analysis approaches  are cluster-level analysis and individual-level analysis using linear mixed models (LMM). Missing outcomes are a commonly  occurring problem in SW-CRTs which can lead to invalid and misleading inferences if ignored or handled inappropriately. In SW-CRTs, it is plausible to have missing outcomes  depending on baseline covariates. In this paper, we considered only continuous outcomes and missingness only in outcomes depending on baseline covariates. We investigate analytically and by simulations the validity of cluster level analysis and LMM using complete record analysis (CRA) and multiple imputed data sets. Cluster level analysis using CRA gives a biased estimate unless the missingness mechanism is the same between the two intervention groups. LMM using CRA gives valid estimate regardless of the missingness mechanisms is the same or different between the intervention groups. On the basis of the simulation study and analytical results, we give guidance on the conditions under which each approach is valid. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="4" data-target=".4collapsed">
            <td>Michael T Fahey</td>
            <td>Longitudinal analysis of heterogeneity in rate of change in PSA level among men with low-grade prost</td>
            <td>12:00 - 12:20</td>
            <td></td>
          </tr>
          <tr class="out budgets 4collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Men diagnosed with low-grade prostate cancer (CAP) may be offered active surveillance (AS) instead of active treatment, which can have adverse side effects on quality of life.  Men on AS are monitored with a regimen of follow-up tests.  Low-grade CAP typically has a long natural history and rapid progression to advanced disease is rare.  However, about 30% of men discontinue AS within five years to have active treatment.  The prostate specific antigen (PSA) test measures the level of a protein in blood and indicates activity in the prostate.  It is not invasive and is inexpensive.<p>This study aims to: 1) estimate the rate of change in PSA level among men on AS, and 2) investigate whether there is heterogeneity in PSA trajectories.<p>279 men in an AS registry in Victoria were studied.  Mean follow-up was 3.5 years and PSA results were available for a median of 11 occasions per man (IQR: 6-18).<p>Longitudinal analysis involved regression of PSA level on time since diagnosis and adjusted for age at diagnosis.  A structural equations modelling approach was used to allow random variation in initial PSA level and in the rate of change of PSA level.  Latent classes in the random coefficients were estimated to investigate heterogeneity.<p>For most men, PSA level increased slowly and the rate of change was approximately 1.5 ng/ml per 5 years.  However, there was evidence that among a small subset of men (18%), the rate of change was five times greater.<p>The existance of a sub-group of men on AS with rapidly increasing PSA levels could indicate the need for a more frequent surveillance regimen and/or for targeted treatment.  Future work needs to focus on validation of this sub-group.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="5" data-target=".5collapsed">
            <td>Graham Hepworth</td>
            <td>Estimation of proportions by group testing with retesting of positive groups</td>
            <td>12:20 - 12:40</td>
            <td></td>
          </tr>
          <tr class="out budgets 5collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          In group testing (or pooled testing), material from individuals is pooled and tested in aggregate for the presence of an attribute, usually a disease.  Group testing for estimation of a proportion p has been applied in a wide range of fields, including virus prevalence in flowers, blood testing (especially for HIV), and transmission of viruses by mosquitoes.  Improved precision usually requires the testing of more groups, but in some situations it is difficult or expensive to obtain the required additional individuals.  If the testing procedure is non-destructive, retesting of groups comprising different combinations of individuals may be a useful option. <p>Hepworth & Watson (2017) developed an estimator of p for the retesting of a random grouping of individuals from the positive groups at the first stage.  The analytic complexity of this estimator led them to use simulation to examine its variance properties.  We have developed two closed-form analytic expressions for the variance of the second-stage estimator, and compared its performance with the results from the simulation.<p>Our analytical solutions give acceptable approximations in a reasonable range of circumstances.  They are most acceptable when the number of groups is not small and p is not large.  This is a useful result for group testing, which to be of major benefit, relies on a reasonable number of groups and a small to moderate prevalence.<p>Hepworth G & Watson RK (2017) Revisiting retesting in the estimation of proportions by group testing.  Communications in Statistics – Simulation and Computation 46:261–274.<p>Hepworth G & Walter SD (2019) Estimation of proportions by group testing with retesting of positive groups.  Communications in Statistics – Theory and Methods DOI:10.1080/03610926.2019.1620280). <br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
    <!--                 -->
    <!----Second Stream --->
    <!--                 -->
    <h2>Contributed Talks Session 1b: Modelling the Environment</h2>
    <h4><i>Room: Exhibition Hall</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive tg-wrap">
      <table class="abstract-table">
        <thead>
          <tr>
            <th>Presenter</th>
            <th>Abstract Title</th>
            <th>Time</th>
            <th>Slides</th>
          </tr>
        </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="1" data-target=".1collapsed">
            <td>Rune Christiansen</td>
            <td>Towards causal inference for spatio-temporal data: adjusting for time-invariant latent confounders</td>
            <td>11:00 - 11:20</td>
            <td></td>
          </tr>
          <tr class="out budgets 1collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          In statistical causality, we are interested not only in modeling the behaviour of a system that is passively observed, but also how the system reacts to changes in the data generating mechanism. Given knowledge of the underlying causal structure, such interventional behaviour can often be estimated from purely observational data (e.g., using covariate adjustment). Typically, the assumption is that data are generated as independent replications from the same underlying mechanism — an assertion that is often hard to justify in practice: geographically varying conditions in which the system is embedded induce spatial heterogeneity, and close-by observations (in space and time) tend to be strongly dependent. In this talk, I present causal models for spatio-temporal data that are adapted to these characteristics, and introduce a simple approach that allows for the estimation of causal effects under the influence of arbitrarily many latent confounders, as long as these confounders do not vary across time. Non-parametric hypothesis tests for the existence of causal effects are constructed based on data resampling, and do not rely on any distributional assumptions on the spatial dependence structure of the data. The method is applied to the problem of inferring the (potential) causal relationship between armed conflict and tropical forest loss, based on a spatio-temporal data set from Colombia. This talk does not require any prior knowledge in causal inference. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="2" data-target=".2collapsed">
            <td>Francis Hui</td>
            <td>Spatial Confounding in GEEs – Why the Working Correlation Matters (well, sort of)</td>
            <td>11:20 - 11:40</td>
            <td></td>
          </tr>
          <tr class="out budgets 2collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Generalized Estimating Equations (GEEs) are a popular tool in many scientific disciplines for investigating the effects of covariates on the mean of a response. In the context of spatial data analysis, GEEs rely on specifying a regression model for the marginal mean, a variance function, and a spatial working correlation matrix characterizing the spatial autocorrelation between observational units. One of the main advantages of GEEs is that estimation of the covariate effects is robust to misspecification of the choice of (spatial) working correlation matrix: the choice only affects the efficiency of the estimator. <p>In ongoing research, we investigate the impact of spatial confounding in GEEs. That is, what happens when the covariates included in a GEE, where a spatial working correlation matrix is used, are also spatially correlated. Under the conditional mixed model approach, the issue of spatial confounding is explicit and arises due to artificial multicollinearity between the spatially correlated covariates and the spatial random effect. We show that for GEEs, such multicollinearity also arises but occurs implicitly between spatially correlated covariates and the spatial working correlation matrix. Results suggests different choices of the working correlation matrix can lead to different attributions of the effect of the covariate on the mean versus on the spatial correlation i.e., on the first versus second moment. In turn, we consider using a so-called “restricted spatial working correlation matrix” that ensures all the variability in the direction of the covariates is attributed to the marginal mean, and is more in line with the underlying aim of GEEs. The issue of standard error estimation via the sandwich covariance matrix, and how it is impacted by spatial confounding, will also be discussed.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="3" data-target=".3collapsed">
            <td>Emy Guilbault</td>
            <td>Fitting species distribution models with uncertain species labels using point process models.</td>
            <td>11:40 - 12:00</td>
            <td></td>
          </tr>
          <tr class="out budgets 3collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          The popularity of species distribution models (SDMs) and the suite of tools to fit them have greatly increased over the last decade. The most common type of species data is presence-only data, which comes from citizen science. Point process models (PPMs) provide a flexible way to fit SDMs to presence-only data. Nonetheless, the quality of presence-only records (both identification and positional location) that serve as input to presence-only SDMs can be questioned. However most species distribution modelling methods applied to presence-only data assume certainty about species identity, but there are many practical situations in which this may not be the case. As an example, observers can be unable to clearly differentiate close species and taxonomists can split a species into multiple distinct species. We investigate the latter case, in which the species identities of records prior to a taxonomic change are confounded. In this talk, I will present two new tools for accommodating confounded records in PPMs. With these tools, we reclassify and incorporate records with uncertain species identities via finite mixture modelling or an iterative algorithm inspired by machine learning methods. Through simulation, we compare performance in classification and in prediction of these tools with different implementations to a standard approach which uses only records with known species labels, varying species abundance, correlation among species distributions, and the proportion of records with missing species labels. We also apply the best-performing methods to fit the distribution of 3 species of native Australian frogs that belong to the genus Myxophies. Among these species, 2 were newly described in 2006 in the northern range of the genus distribution and thus confounded previous records in the area made over the previous few decades.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="4" data-target=".4collapsed">
            <td>Ian Renner</td>
            <td>Modelling species communities using presence-only data</td>
            <td>12:00 - 12:20</td>
            <td></td>
          </tr>
          <tr class="out budgets 4collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Often, the only available data to serve as input for species distribution models is presence-only data, which consists of observation records for the target species with no corresponding absence information. While a number of presence-only species distribution modelling methods can relate the occurrence patterns to the environment, there are limits to the types of inference that can be made from presence-only data. Indeed, there are a number of biotic and abiotic factors that influence the distribution of species and the composition of species communities, such as species co-occurrence patterns, traits, and phylogenetic relationships among species, and it is presently difficult to incorporate these factors into presence-only models. <p>With other data inputs such as presence-absence and count data, such inference about species and species communities is available via the hierarchical modelling of species communities (HMSC) platform. With HMSC, users can partition the variation in species occurrences to components that relate to environmental filtering, species interactions, and random processes, allowing for both species-level and community-level inference. However, the HMSC package does not currently support presence-only input.<p>In this talk, I will present developments of presence-only input into the HMSC framework and demonstrate the new types of inference available as a result. These developments not only enable such inference for presence-only data, but likewise allow previous analyses of species communities from HMSC to be enriched by incorporating relevant presence-only data.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="5" data-target=".5collapsed">
            <td>Louise McMillan</td>
            <td>TBC</td>
            <td>12:20 - 12:40</td>
            <td></td>
          </tr>
          <tr class="out budgets 5collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          TBC<br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
    <!--                -->
    <!----First Stream --->
    <!--                -->
  <h2>Contributed Talks Session 2a: Imputation</h2>
    <h4><i>Room: The Gallery</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse2" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive  tg-wrap">
      <table class="abstract-table">
        <thead>
          <tr>
            <th>Presenter</th>
            <th>Abstract Title</th>
            <th>Time</th>
            <th>Slides</th>
          </tr>
        </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="6" data-target=".6collapsed">
            <td>Nidhi Menon</td>
            <td>The effect of number of clusters and cluster sizes on multiple imputation in multilevel models</td>
            <td>16:00 - 16:20</td>
            <td></td>
          </tr>
          <tr class="out budgets 6collapsed collapse multi-collapse2" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Missing data are a common phenomenon in public health research. Multiple Imputation (MI) has been long recognized as an attractive approach to handle missing values. Statisticians are now advocating the use of MI as a gold standard in solving the missing data problem. Despite its early conception and its numerous advantages over the traditional ad hoc methods, there is still limited application of MI in public health research. <p>The theory of multiple imputation requires that imputations be made conditional on the sampling design. Not accounting for complex sample design features, such as stratification and clustering, during imputations can yield biased estimates from a design-based perspective. <p>Most datasets in public health research show some form of natural clustering (individuals within households, households within the same district, patients within wards, etc.).  Cluster effects are often of interest in health research. In this study, we investigate through simulations different strategies for accounting for clustering when multiply imputing variables. Recent studies have identified methods to include fixed effects for clusters in imputations, however there is limited information on impact of varying number of clusters and cluster sizes on MI. <p>In this study, we simulate 3 level hierarchical data structures varying the number of clusters at each level. Missing values are present in covariates at each level in the data. We consider the impact of the combination of varying cluster sizes and proportion of missingness in imputation of covariates at each level in the dataset. This study implements the Gelman and Hill approach for imputation of missing data at higher levels by including aggregate forms of individual level measurements to impute for missing values at higher levels. The performance of popular methods of imputations, MICE and JoMo are compared. Performance measures include bias in estimates, mean squared errors and probability coverage of confidence intervals<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="7" data-target=".7collapsed">
            <td>Thomas Sullivan</td>
            <td>Multiple imputation for missing outcome data in trials involving independent and paired observations</td>
            <td>16:20 - 16:40</td>
            <td></td>
          </tr>
          <tr class="out budgets 7collapsed collapse multi-collapse2" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Background: Trials involving a mixture of independent and paired data arise in many areas of health research, for example in paediatrics where outcomes can be collected on singletons and twins, and in ophthalmology, where one or both eyes may require treatment. An important consideration in these trials is the correlation in outcomes, or clustering, that occurs between observations from the same pair. When applying multiple imputation (MI) to address missing data, previous research suggests that any clustering in the data should be accounted for in the imputation and analysis models. However, most ad-hoc methods of MI for clustered data were designed with large and/or equal sized clusters in mind, and it is unclear how MI should be imeplemented in settings with independent and paired data.<p>Methods: Using simulated data the following MI approaches were evaluated: (1) MI ignoring clustering; (2) MI using chained equations with conditional imputation of the 2nd member of a pair; (3) MI performed separately by cluster size, and; (4) multi-level MI. Observations were allocated to one of two treatment groups using simple randomisation, with members of a pair randomised individually, to the same (cluster randomisation) or to opposite groups (opposite randomisation). <p>Results: When outcome data were missing at random, all MI methods produced unbiased treatment effect estimates. Although performance deficits were small, MI ignoring clustering and chained equations with conditional imputation produced confidence intervals for the treatment effect that were too narrow under cluster randomisation and too wide under opposite randomisation. MI performed separately by cluster size and multi-level MI performed well across the range of scenarios considered.<p>Conclusions: In trials involving a mixture of independent and paired observations, particularly those employing cluster or opposite randomisation, we recommend researchers apply multi-level MI or standard MI performed separately by cluster size to address missing outcome data.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="8" data-target=".8collapsed">
            <td>Patrick Graham</td>
            <td>Adjusting for linkage bias in the analysis of record linked data.</td>
            <td>16:40 - 17:00</td>
            <td></td>
          </tr>
          <tr class="out budgets 8collapsed collapse multi-collapse2" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Data formed the application of record linkage techniques to two or more datasets are increasingly important in public health and social science research. Regardless of the linkage method, it is commonly the case that not all records can be linked. If linkage rates vary by variables relevant to an analysis then analyses restricted to only linked records may be biased. Records in a base dataset that are not linked to a record in target dataset will have missing values for all variables recorded only on the target dataset. This produces a block missing-ness structure similar to that encountered in panel studies when study members decline participation in one or more study waves.  While missing data theory and methods are clearly applicable to this problem it is insightful to work through the specifics of missing data methodology as it applies to linkage bias. Both Bayesian and frequentist approaches to the problem are considered.  In certain circumstances complete-case analyse can be justified, though this depends on the role the partially observed variables play in analysis (e.g. outcome or exposure).  When complete-case analyses are not justified, the available analysis methods can be classified into those that condition on both linked and unlinked records and methods based only on linked cases with some adjustment for incomplete linkage.  The Bayesian approach leads naturally to the former perspective and leads to Gibbs sampling and multiple imputation as reasonable methods. Analytic approaches based on adjusted complete-case analyses fit best within a frequentist framework, and conditional likelihood and inverse probability weighting methods appear reasonable options.  A simulation study   confirms that multiple imputation, conditional likelihood and inverse probability weighting methods all adjust appropriately for linkage bias and achieve nominal interval coverage rates. Multiple imputation is generally more efficient that conditional likelihood or inverse probability weighting methods. <br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
    <!--                 -->
    <!----Second Stream --->
    <!--                 -->
    <h2>Contributed Talks Session 2b: Experimental Design</h2>
    <h4><i>Room: Exhibition Hall</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse2" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive  tg-wrap">
      <table class="abstract-table">
      <thead>
        <tr>
          <th>Presenter</th>
          <th>Abstract Title</th>
          <th>Time</th>
          <th>Slides</th>
        </tr>
      </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="6" data-target=".6collapsed">
            <td>Nidhi Menon</td>
            <td>The effect of number of clusters and cluster sizes on multiple imputation in multilevel models</td>
            <td>16:00 - 16:20</td>
            <td></td>
          </tr>
          <tr class="out budgets 6collapsed collapse multi-collapse2" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Missing data are a common phenomenon in public health research. Multiple Imputation (MI) has been long recognized as an attractive approach to handle missing values. Statisticians are now advocating the use of MI as a gold standard in solving the missing data problem. Despite its early conception and its numerous advantages over the traditional ad hoc methods, there is still limited application of MI in public health research. <p>The theory of multiple imputation requires that imputations be made conditional on the sampling design. Not accounting for complex sample design features, such as stratification and clustering, during imputations can yield biased estimates from a design-based perspective. <p>Most datasets in public health research show some form of natural clustering (individuals within households, households within the same district, patients within wards, etc.).  Cluster effects are often of interest in health research. In this study, we investigate through simulations different strategies for accounting for clustering when multiply imputing variables. Recent studies have identified methods to include fixed effects for clusters in imputations, however there is limited information on impact of varying number of clusters and cluster sizes on MI. <p>In this study, we simulate 3 level hierarchical data structures varying the number of clusters at each level. Missing values are present in covariates at each level in the data. We consider the impact of the combination of varying cluster sizes and proportion of missingness in imputation of covariates at each level in the dataset. This study implements the Gelman and Hill approach for imputation of missing data at higher levels by including aggregate forms of individual level measurements to impute for missing values at higher levels. The performance of popular methods of imputations, MICE and JoMo are compared. Performance measures include bias in estimates, mean squared errors and probability coverage of confidence intervals<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="7" data-target=".7collapsed">
            <td>Thomas Sullivan</td>
            <td>Multiple imputation for missing outcome data in trials involving independent and paired observations</td>
            <td>16:20 - 16:40</td>
            <td></td>
          </tr>
          <tr class="out budgets 7collapsed collapse multi-collapse2" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Background: Trials involving a mixture of independent and paired data arise in many areas of health research, for example in paediatrics where outcomes can be collected on singletons and twins, and in ophthalmology, where one or both eyes may require treatment. An important consideration in these trials is the correlation in outcomes, or clustering, that occurs between observations from the same pair. When applying multiple imputation (MI) to address missing data, previous research suggests that any clustering in the data should be accounted for in the imputation and analysis models. However, most ad-hoc methods of MI for clustered data were designed with large and/or equal sized clusters in mind, and it is unclear how MI should be imeplemented in settings with independent and paired data.
          Methods: Using simulated data the following MI approaches were evaluated: (1) MI ignoring clustering; (2) MI using chained equations with conditional imputation of the 2nd member of a pair; (3) MI performed separately by cluster size, and; (4) multi-level MI. Observations were allocated to one of two treatment groups using simple randomisation, with members of a pair randomised individually, to the same (cluster randomisation) or to opposite groups (opposite randomisation). 
          Results: When outcome data were missing at random, all MI methods produced unbiased treatment effect estimates. Although performance deficits were small, MI ignoring clustering and chained equations with conditional imputation produced confidence intervals for the treatment effect that were too narrow under cluster randomisation and too wide under opposite randomisation. MI performed separately by cluster size and multi-level MI performed well across the range of scenarios considered.
          Conclusions: In trials involving a mixture of independent and paired observations, particularly those employing cluster or opposite randomisation, we recommend researchers apply multi-level MI or standard MI performed separately by cluster size to address missing outcome data.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="8" data-target=".8collapsed">
            <td>Patrick Graham</td>
            <td>Adjusting for linkage bias in the analysis of record linked data.</td>
            <td>16:40 - 17:00</td>
            <td></td>
          </tr>
          <tr class="out budgets 8collapsed collapse multi-collapse2" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Data formed the application of record linkage techniques to two or more datasets are increasingly important in public health and social science research. Regardless of the linkage method, it is commonly the case that not all records can be linked. If linkage rates vary by variables relevant to an analysis then analyses restricted to only linked records may be biased. Records in a base dataset that are not linked to a record in target dataset will have missing values for all variables recorded only on the target dataset. This produces a block missing-ness structure similar to that encountered in panel studies when study members decline participation in one or more study waves.  While missing data theory and methods are clearly applicable to this problem it is insightful to work through the specifics of missing data methodology as it applies to linkage bias. Both Bayesian and frequentist approaches to the problem are considered.  In certain circumstances complete-case analyse can be justified, though this depends on the role the partially observed variables play in analysis (e.g. outcome or exposure).  When complete-case analyses are not justified, the available analysis methods can be classified into those that condition on both linked and unlinked records and methods based only on linked cases with some adjustment for incomplete linkage.  The Bayesian approach leads naturally to the former perspective and leads to Gibbs sampling and multiple imputation as reasonable methods. Analytic approaches based on adjusted complete-case analyses fit best within a frequentist framework, and conditional likelihood and inverse probability weighting methods appear reasonable options.  A simulation study   confirms that multiple imputation, conditional likelihood and inverse probability weighting methods all adjust appropriately for linkage bias and achieve nominal interval coverage rates. Multiple imputation is generally more efficient that conditional likelihood or inverse probability weighting methods. <br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
  <div id="wednesday" class="tab-pane fade">
    <!-- Wed -->
    <!-- <div class="table-responsive"> -->
    <!--                -->
    <!----First Stream --->
    <!--                -->
    <h2>Contributed Talks Session 1a: Methods I</h2>
    <h4><i>Room: Vines</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive tg-wrap">
      <table class="abstract-table">
        <thead>
          <tr>
            <th>Presenter</th>
            <th>Abstract Title</th>
            <th>Time</th>
            <th>Slides</th>
          </tr>
        </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="1" data-target=".1collapsed">
            <td>Paul Kabaila</td>
            <td>Confidence intervals centred on bootstrap smoothed estimators</td>
            <td>8:50 - 9:10</td>
            <td></td>
          </tr>
          <tr class="out budgets 1collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Bootstrap smoothed (or bagged; Breiman, 1996) parameter estimators have been proposed as an improvement on estimators found after preliminary data-based model selection. The key result of Efron (2014) is a formula for a delta method approximation to the standard deviation of the bootstrap smoothed estimator. This formula is valid for any exponential family of models and has the attractive feature that it simply re-uses the parametric bootstrap replications that were employed to find this estimator. It also has the attractive feature that it is applicable in the context of complicated data-based model selection. It is natural then to propose the use of a confidence interval that is centred on the bootstrap smoothed estimator and has width proportional to the estimate of this approximation to the standard deviation. We describe the results of an evaluation of the performance of this confidence interval, using a testbed that consists of two nested linear regression models and preliminary model selection using a t-test.<p> References<p>Breiman, L. (1996) Bagging predictors. Machine Learning.<p>Efron, B. (2014) Estimation and accuracy after model selection. Journal of the American Statistical Association.<p>Kabaila, P. and Wijethunga, C. (2019) Confidence intervals centred on bootstrap smoothed estimators. Australian & New Zealand Journal of Statistics.<p>Kabaila, P. and Wijethunga, C. (2019) On confidence intervals centred on bootstrap smoothed estimators. Stat. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="2" data-target=".2collapsed">
            <td>Fernando Marmolejo-Ramos</td>
            <td>Towards distributional analyses of biomarker data</td>
            <td>9:10 - 9:30</td>
            <td></td>
          </tr>
          <tr class="out budgets 2collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Current literature reviews of published datasets in the cognitive sciences (e.g. psychology, education, neuroscience) indicate that normally distributed data is not the norm but the exception. One such type of data is biomarker data. In a nutshell, biomarkers can be redefined as measures that index changes in neuropsychobiological states. This definition hence includes, among others, measures such as reaction times, event-related potentials, thermographic data, and blood pressure. Canonical statistical methods (e.g. t-tests, ANOVA) give biased results when dealing with non-normal data. The goal of this talk is to provide a snapshot of new statistical techniques that allow proper distributional and robust analyses. The emphasis will be on statistical graphics and conceptual definitions rather than mathematical elaborations. It is argued that adopting these techniques will ultimately lead to novel findings by revamping the way in which biomarker data are explored and analysed.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="3" data-target=".3collapsed">
            <td>Linh Nghiem</td>
            <td>Simulation-Selection-Extrapolation: Estimation in High Dimensional Errors-in-Variables Models</td>
            <td>9:30 - 9:50</td>
            <td></td>
          </tr>
          <tr class="out budgets 3collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Errors-in-variables models in high-dimensional settings pose two challenges in application. Firstly, the number of observed covariates is larger than the sample size, while only a small number of covariates are true predictors under an assumption of model sparsity. Secondly, the presence of measurement error can result in severely biased parameter estimates, and also affects the ability of penalized methods such as the lasso to recover the true sparsity pattern. A new estimation procedure called SIMSELEX (SIMulation-SELection-EXtrapolation) is proposed. This procedure makes double use of lasso methodology. Firstly, the lasso is used to estimate sparse solutions in the simulation step, after which a group lasso is implemented to do variable selection. The SIMSELEX estimator is shown to perform well in variable selection, and has significantly lower estimation error than naive estimators that ignore measurement error. SIMSELEX can be applied in a variety of errors-in-variables settings, including linear models, generalized linear models, and Cox survival models. It is furthermore shown in the supporting information how SIMSELEX can be applied to spline-based regression models. A simulation study is conducted to compare the SIMSELEX estimators to existing methods in the linear and logistic model settings, and to evaluate performance compared to naive methods in the Cox and spline models. Finally, the method is used to analyze a microarray dataset that contains gene expression measurements of favorable histology Wilms tumors. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="4" data-target=".4collapsed">
            <td>Brenton R Clarke</td>
            <td>Trimmed Estimators - and a Hybrid-Censored Data Approach to Estimation</td>
            <td>9:50 - 10:10</td>
            <td></td>
          </tr>
          <tr class="out budgets 4collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
         Using the idea of trimmed likelihood in location, we revisit the functional form of the trimmed mean and specify the functional form of the trimmed likelihood estimator as per Bednarski and Clarke (1993).<br>This definition leads in the case of the exponential distribution to a naturally robust estimator which is highly efficient whose functional form is weakly continuous and Fr&eacute;chet differentiable at the exponential model. See Clarke et al (2000). This is known as the &beta;-trimmed mean.  But what about censored data?  Should we use the maximum likelihood estimator (mle) because censoring  seemingly implies outliers are automatically taken care of?  We explore a proposal of a hybrid estimator that combines the &beta;-trimmed mean and the mle for some interesting results. See Clarke et al. (2017).<br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="5" data-target=".5collapsed">
            <td>Michael Stewart</td>
            <td>Robust scale estimation under small measurement errors</td>
            <td>10:10 - 10:30</td>
            <td></td>
          </tr>
          <tr class="out budgets 5collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Our motivating example is the estimation of robust scale functionals (like interquartile range or median absolute deviation) of the latent distribution in random effect models. This can be cast as a measurement error problem, where we "estimate" the random effects associated with each cluster. These estimates can in turn be regarded as the true random effects, plus some measurement errors. By "small measurement errors" we mean that we adopt a particular asymptotic scheme where both the number of clusters, and their sizes, tend to infinity. We propose to apply a "missing information principle" whereby we approximate the conditional expectation of the semiparametrically efficient score equations, given the observed data. We report some very interesting simulation study results and briefly sketch some accompanying theory. <br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
    <!--                 -->
    <!----Second Stream --->
    <!--                 -->
    <h2>Contributed Talks Session 1b: Mixed Models in Agriculture</h2>
    <h4><i>Room: Exhibition Hall</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive tg-wrap">
      <table class="abstract-table">
        <thead>
          <tr>
            <th>Presenter</th>
            <th>Abstract Title</th>
            <th>Time</th>
            <th>Slides</th>
          </tr>
        </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="1" data-target=".1collapsed">
            <td>Bethany Macdonald</td>
            <td>Performance of factor analytic models in multi-environment trial data with small variety numbers</td>
            <td>8:50 - 9:10</td>
            <td></td>
          </tr>
          <tr class="out budgets 1collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Much of the research undertaken in crop science involves evaluating varieties in field trials spanning a range of years and locations, where data arising from these sets of trials are known as multi-environment trial (MET) data. Analysis methods for MET data are concerned with investigating the nature of the variety by environment (VxE) effects, which describe the performance of different varieties across different environments, often with the aim of determining those varieties with superior performance. Ideally the variance of the VxE effects would be estimated using an unstructured form, assumed to be of full rank, however, this estimation is computationally difficult and can be unstable. The factor analytic (FA) model proposed by Smith et al. (2001) offers a more parsimonious option and has been shown to provide a good approximation to the unstructured form. The FA model has been shown to model the VxE effects accurately when there are large numbers of varieties, however, the accuracy of the model when variety numbers are small or varietal concurrence (the number of varieties in common between trials) is varied is unclear. The accuracy of FA models in scenarios when variety numbers are small and VxE and concurrence patterns vary was investigated through simulation. The study considered four model types for the variance of the VxE effects for MET data, with 10, 15, 25 and 50 varieties per trial. These data sets contained three types of VxE patterns, two levels of varietal concurrence and nine or 52 trials, resulting in 48 different scenarios. This study found that the accuracy of the FA model is affected when the number of varieties per trial are small, but the extent is dependent on other factors such as the number of trials and underlying VxE pattern.  <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="2" data-target=".2collapsed">
            <td>Bethany Rognoni</td>
            <td>An evaluation of separable variance structures for highly genetically correlated environments</td>
            <td>9:10 - 9:30</td>
            <td></td>
          </tr>
          <tr class="out budgets 2collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          In agricultural field trials, genotypes can be tested in the presence of interacting factors, resulting from either imposed treatments, repeated measurements or across multiple environments. When the aim is to select superior performing genotypes under these conditions, the factorial combination of conditions is often collapsed into one overall ‘environment’ factor, with a genetic variance structure fitted for this term, in a linear mixed model framework. This enables the estimation of a separate genetic variance for each of the levels (referred to as environment types) corresponding to the combinations of the original factorial structure, along with genetic covariance between the environment types. <p>As an alternative to this, an individual genetic variance structure for each of the factors can be fitted, resulting in a separable variance model. This may be a more parsimonious approach to modelling the genetic variance due to estimation of fewer parameters in total, and could provide a more intuitive interpretation of genotype behaviour across environment types. However, separable models have been shown to be less flexible, as they impose more restrictive variance structures. <p>A set of nematode resistance field trials motivated the exploration of separable variance models, where final nematode counts for each genotype were measured under three different soil depths, two previous nematode population densities resulting from prior crops and three trials. The analysis was first conducted using an overall environment model, where each environment represented a unique combination of soil depth, nematode population and trial. This model showed consistently high genetic correlations between all pairs of environments. The analysis was then performed using a separable genetic variance structure for the three-way factorial. There were substantial differences in the genetic variances and correlations that resulted from the different parameterisation of both models. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="3" data-target=".3collapsed">
            <td>Isabel Munoz-Santa</td>
            <td>Multi environment trial analysis to determine the tolerance of cereal varieties to cyst nematode</td>
            <td>9:30 - 9:50</td>
            <td></td>
          </tr>
          <tr class="out budgets 3collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Cereal Cyst nematodes (CCN) are considered a major nematode pest which cause significant yield losses in wheat and barley with a high economic cost to the grain industry.  A feasible solution is to breed for varieties which are tolerant; i.e. varieties which are able to yield in the presence of CCN in the field, this being an important research area for cereal nematologists.<p>In this study, we conducted 6 field trials from 2011 to 2018 in South Australia and Victoria with the objective of providing tolerance ratings of 92 different cereal varieties under high and low levels of pre-established nematode densities in the field.  Multi environment trial analyses were used to analyse the data where the term environment refers to each year by location by nematode density combination. Spatial techniques were used to account for the spatial variability in each trial and a factor analytic model was fitted to model the genotype by environment interaction effects. <p>Multi environment trial analyses revealed high genetic correlation between high and low environments for each of the trials. This indicates that selecting varieties under high levels of CCN is equivalent to select varieties under low levels and thus analogous to select high yielding varieties irrespective of the nematode density. Therefore, a measure to assess the performance of varieties under high levels of nematodes independently of their performance under low levels was defined and named “tolerance index”.<p>Multi environment trial analyses together with the tolerance index allowed us to select for high yielding varieties as well as give a more useful information to growers in relation to the comparison of varieties between high and low levels of CCN. The definition of the tolerance index and results obtained from this set of trials will be presented in this talk. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="4" data-target=".4collapsed">
            <td>Clayton Forknall</td>
            <td>An across trials random regression approach to describe tolerance of wheat cultivars to disease</td>
            <td>9:50 - 10:10</td>
            <td></td>
          </tr>
          <tr class="out budgets 4collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          In plant pathology, tolerance to disease can be defined as the rate of change in productivity (yield), given a unit increase in pathogen burden. When applied to field crops, experiments to compare the tolerance of cultivars requires the establishment of a range of pathogen burdens over which the grain yield of a cultivar is measured, where tolerance to disease is then defined as the slope of a regression of yield against pathogen burden. <p>A recent publication (Forknall et al. (2019), Phytopathology) describes a method for the statistically robust design and analysis of a single field experiment to quantify the rate of change in yield per unit increase in pathogen burden of five wheat cultivars for the disease crown rot. Using a random regression approach, implemented in a linear mixed model (LMM) framework, response curves describing the relationship between yield and crown rot pathogen burden were estimated for each of the cultivars in the experiment. <p>This methodology is now extended to an across trials random regression approach, implemented in an LMM framework, which enables the estimation of different response curves for each cultivar in each experiment, where the response of each cultivar is modelled around an overall (average) yield response profile for each experiment. This modelling approach also provides a means of capturing the genetic correlation (covariance) between model parameters, both within and between experiments. <p>The model is demonstrated by an application to 15 field experiments, estimating the yield response of the same set of five wheat cultivars to crown rot. The analysis revealed variation in the rate of change in yield, or tolerance, of cultivars across experiments, identifying that the tolerance of some cultivars was more influenced by environmental conditions than others. Also presented are graphical tools to assist in unlocking the interaction between cultivars tolerance and environmental conditions. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="5" data-target=".5collapsed">
            <td>Michael Mumford</td>
            <td>Clustering environments in a combined trial analysis of yield response to plant population</td>
            <td>10:10 - 10:30</td>
            <td></td>
          </tr>
          <tr class="out budgets 5collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          One of the key management practices of interest in agricultural crops is yield response to plant population and its contribution to yield in different genetic backgrounds. When field trials are run across multiple environments (i.e. years and locations), it is imperative to model the environment effects and the interaction of environment with hybrid and plant population. The challenge is then to compare the response models across environments. <p>A fixed effects combined trial analysis for yield response to plant population in a linear mixed model framework is presented. Clustering of the treatments (e.g. hybrid and environment combinations) is performed such that parallel curves are fitted to treatments within the same cluster to i) simplify the interaction and ii) increase the information used for fitting each response function by combining data across treatments within the same cluster. The categorisation of treatments to clusters is considered adequate when there is no significant interaction effect between plant population and treatment within each cluster. This presentation will focus on an objective method for categorising treatments into clusters. Furthermore, a sensitivity analysis will be presented to explore the adequacy of the proposed clustering methodology.<p>The methodology is applied to a large set of sorghum trials implemented across New South Wales, Australia in two seasons consisting of different hybrids, row spacings and trial locations. Furthermore, it provides a general framework for a fixed effects regression analysis in a linear mixed model framework that can account for i) experimental design terms, ii) separate residual variances and iii) spatial field trend at each trial. <br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
    <h2>Invited Speakers: Environmental</h2>
    <h4><i>Room: Exhibition Hall</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive">
      <table class="abstract-table">
        <thead>
          <tr>
            <th>Presenter</th>
            <th>Abstract Title</th>
            <th>Time</th>
            <th>Slides</th>
          </tr>
        </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="9" data-target=".9collapsed">
            <td>Christopher K. Wikle</td>
            <td>Using Deep Neural Models to Facilitate Statistical Modeling of Complex Spatio-Temporal Dynamics</td>
            <td>11:00 - 12:00</td>
            <td></td>
          </tr>
          <tr class="out budgets 9collapsed collapse multi-collapse1" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Spatio-temporal data are ubiquitous in the sciences and engineering, and their analysis is important for understanding and predicting a wide variety of processes. One of the difficulties with statistical modeling of spatial processes that change in time is the complexity of the dependence structures that must describe how such a process varies, and the presence of high-dimensional complex datasets and large prediction domains. It is particularly challenging to specify parameterizations for nonlinear dynamic spatio-temporal models (DSTMs) that are simultaneously useful scientifically, efficient computationally, and allow for proper uncertainty quantification.  Alternatively, the machine learning community has developed a suite of deep neural models and learning paradigms (e.g., convolutional neural networks, recurrent neural networks, reinforcement learning) that can be combined in novel ways to predict spatio-temporal processes.  However, these deep neural models are typically implemented in a deterministic framework that limits formal inference.  Here we explore some recent attempts to build hybrid models in which deep neural models can be embedded within a formal statistical DSTM framework. The approaches are illustrated with examples applied to environmental and ecological data.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="10" data-target=".10collapsed">
            <td>Blair Robertson</td>
            <td>Quasi-random spatially balanced sampling</td>
            <td>13:00 - 14:00</td>
            <td></td>
          </tr>
          <tr class="out budgets 10collapsed collapse multi-collapse1" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          A spatial sampling design determines where sample locations are placed in a study area. The main objective is to select sample locations in such a way that valid scientific inferences can be made to all regions of the study area. To achieve good estimates of population characteristics, the spatial pattern of the sample should be similar to the spatial pattern of the population. However, the spatial pattern of the response variable is usually not known. Fortunately, when sampling natural resources, nearby locations tend to be similar because they interact with one another and are influenced by the same set of factors. This means sample efficiency can be increased by spreading sample locations evenly over the resource. A sample that is well-spread over the resource is called a spatially balanced sample. In this talk, we show how quasi-random sequences can be used to draw spatially balanced samples from natural resources. <br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
    <!-- </div> -->
    <!-- Wed -->
  </div>
  <div id="thursday" class="tab-pane fade">
    <!-- Thurs -->
    <!--                -->
    <!----First Stream --->
    <!--                -->
    <h2>Contributed Talks Session 1a: Methods II</h2>
    <h4><i>Room: Vines</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive tg-wrap">
      <table class="abstract-table">
        <thead>
          <tr>
            <th>Presenter</th>
            <th>Abstract Title</th>
            <th>Time</th>
            <th>Slides</th>
          </tr>
        </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="1" data-target=".1collapsed">
            <td>Takeshi Kurosawa</td>
            <td>A new Liu estimator under a linear constraint</td>
            <td>8:50 - 9:10</td>
            <td></td>
          </tr>
          <tr class="out budgets 1collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          A problem in parameter estimation for a linear model arises multi-colinearity if a data set is ill-posed. One of the solutions, Liu (1993) proposed his estimator called the Liu estimator. Furthermore, Kaciranlar et al. (1999) generalized the Liu estimator under the condition with a linear constraint among the parameters. However, their estimator is an ad-hoc method because they treated the two problems independently. In this study, we interpret the Liu estimator as the solution of a certain loss function, and then we propose a new estimator under the condition with the linear constraint. We show theoretical bias and RMSE of our estimator and give a necessary and sufficient condition for the superiority of our estimator against other estimators in terms of RMSE. We also perform a simple simulation study using empirical RMSE. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="2" data-target=".2collapsed">
            <td>Zhanglong Cao</td>
            <td>Model selection and principle of parsimony in statistical modelling in agriculture</td>
            <td>9:10 - 9:30</td>
            <td></td>
          </tr>
          <tr class="out budgets 2collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Model selection is an important issue in biostatistical, psychological and agricultural studies. Root mean squared error (RMSE), Akaike's information criterion (AIC), Bayesian information criterion (BIC) and their relatives are commonly used as selection criteria for goodness-of-fit of statistical models. However, there is no robust technique that can be applied in every aspect of parameter estimation and model selection. Sometimes, the winning model is ``cursed'', while the best model based on the selection criteria leads to over-fitting in practice. Goodness-of-fit must be balanced against model complexity to avoid over-fitting issues. We discuss the trap in model selection and the principle of parsimony, and present a weighted neighbouring cross-validation method. The latter will be illustrated on agricultural experimental data set. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="3" data-target=".3collapsed">
            <td>Hwan-Jin Yoon</td>
            <td>On the effect of dependencies between regressors and random effects when analysing hierarchical structured data</td>
            <td>9:30 - 9:50</td>
            <td></td>
          </tr>
          <tr class="out budgets 3collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Hierarchically structured data arises frequently in practice, in many fields of sciences including social science. Linear mixed models (LMMs) is one of the most common statistical methods to deal with the structured or clustered data.<p> Yoon & Welsh (2019) studied the effect of ignoring clustering in $x$ on fitting LMM, and showed that it can be obtained misleading assessments of both the association between $y$ and $x$ and of the variance components. They also showed that, as the within cluster variance of $x$, $\tau_x$, increases, the likelihood and the REML criterion develop two distinct local maxima and which of these is the global maximum changes at the jump point. <p>In LMMs for clustered or hierarchical structured data, regressors can be correlated with random effects. When the random effects and regressors independence assumptions are violated, not only regression coefficient estimators but also variance components can be severely biased. <p>In this study, we investigate how the violation of the random effects and regressors independence assumption could affect on the estimates of parameters and compare the results with the effect of ignoring clustering in $x$ when fitting LMM (Yoon & Welsh). We also extend our study to the case of correlated binary data. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="4" data-target=".4collapsed">
            <td>Olena Kravchuk</td>
            <td>Comparative study of probability models for agriculture field index data</td>
            <td>9:50 - 10:10</td>
            <td></td>
          </tr>
          <tr class="out budgets 4collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
         This simulation study is motivated by the challenges of the analysis of field indices, like Harvest Index and Nutrient Use Efficiencies, in agriculture research. The index data we are interested in represents the quotient of two positively correlated random variables, X/(X+Y), but is only observed as a single index variate, Z. Based on the analysis of that index, important decisions are made about the ranking of agronomic factors and conditions. In this study, we are concerned with probability models for such indices, considering and contrasting several choices for X and Y. The choices include: normal, Beta and log-normal distributions, as well as mixtures of either Beta or log-normal. We demonstrate that the mixture of log-normal provides an identifiable and flexible model. <br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="5" data-target=".5collapsed">
            <td>Warren Muller</td>
            <td>Modelling and Parameterization of Soil-Water Retention Curves</td>
            <td>10:10 - 10:30</td>
            <td></td>
          </tr>
          <tr class="out budgets 5collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          The prediction of soil water storage and water supply to plants is essential in the investigation of vegetation response to rainfall. In particular, soil water retention curves (WRCs) which depict the relationship between the volumetric water content (VWC) and the soil water potential (h) are used by soil scientists in their studies of soil hydraulic properties. <p>If the shape of the water retention curve is sigmoid, then such data sets can be characterized by several models. The most commonly used model is the van Genuchten model (Soil Sci. Soc. Am. J. 44, 1980, 892-898). <p>Fitting the van Genuchten model to data sets relating VWC to h involves estimation of four parameters, some of which have a physical significance in understanding soil-water relationships. However typically these data sets only have 8 to 10 (VWC, h) pairs, which leads to difficulty in estimating some of the parameters and, in some cases, the van Genuchten model is difficult to fit.<p>We fitted the van Genuchten model to VWC vs. h data for 35 water retention curves from semi-arid rangeland soils and seven vineyard soils from the Yass Valley, NSW. We present examples of these fitted models. Some new parameters were derived from the fitted values, and relationships between the four estimated parameters and these derived parameters are also presented. The results from this study show that<p>
          (i)	On some occasions the van Genuchten model fits poorly or doesn’t fit, so is inappropriate;<br>
          (ii)	There are strong relationships between some of the estimated and derived parameters, meaning the model is most likely over-parametrized;<br>
          (iii)	Alternative models should be used when the VWC vs. h relationship is not a distinct sigmoid shape, for example curvilinear or near linear.
 <br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
    <!--                 -->
    <!----Second Stream --->
    <!--                 -->
    <h2>Contributed Talks Session 1b: Collaboration</h2>
    <h4><i>Room: Exhibition Hall</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive tg-wrap">
      <table class="abstract-table">
        <thead>
          <tr>
            <th>Presenter</th>
            <th>Abstract Title</th>
            <th>Time</th>
            <th>Slides</th>
          </tr>
        </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="1" data-target=".1collapsed">
            <td>Susan Wilson</td>
            <td>Big Biometric Data: A Biostatistical Perspective</td>
            <td>8:50 - 9:10</td>
            <td></td>
          </tr>
          <tr class="out budgets 1collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Technological advances in biology and medicine, ranging from high-throughput sequencing to wearable electronic devices, are producing a “tsunami” of big biometric data. These data are of very widely varying types and quality. Many challenges abound on how to deal with such data, including wide-ranging deliberations concerning statistical modelling. This presentation will give an overview, including some current developments to meet the more pressing of these challenges. As well, in medicine in particular, big biometric data are giving rise to subtle and evolving ethical issues, many of which concern data analysts. Particular attention will be given to how these affect modern developments including applications such as personalised medical treatment. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="2" data-target=".2collapsed">
            <td>Teresa Neeman</td>
            <td>Bridging the gap between science and statistics</td>
            <td>9:10 - 9:30</td>
            <td></td>
          </tr>
          <tr class="out budgets 2collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Biology has undergone a huge transformation in the last 50 years from an observational science with occasional data to an experimental science with terabytes of data. The computational challenges in visualising and analysing large biological data sets are formidable for most biologists who trained in more data-naïve environments. This is potentially a huge opportunity for statisticians, whose training can help biologists discover mechanisms, patterns of behaviour, and even new biological paradigms. But training research students often occurs in “silos” in both fields, leaving huge communication gaps between the biological sciences and statistics.  Biologists miss the chance to sophisticated statistical machinery that may elucidate biological functions. Statisticians miss out on discovering the power of data to address important questions.  In this talk, I explore how we can start to bridge this communication gap. As statisticians, we need to teach biologists statistical concepts that are relevant to experimental sciences. These concepts need to emphasise experimental design and mixed effects models. We need to challenge ourselves to learn the language of biologists and challenge our statistics students with real-life complex data problems. Finally, we need to connect research students with one another and encourage research collaborations. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="3" data-target=".3collapsed">
            <td>Sharon Nielsen</td>
            <td>Statistics for agronomists: a constructive synthesis of workplace learning and community of practice</td>
            <td>9:30 - 9:50</td>
            <td></td>
          </tr>
          <tr class="out budgets 3collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Stakeholders in agronomy, and many other discipline areas, are promoting reproducible and transparent research, allowing research results to be replicated. The need for scientific rigour is paramount to the agronomic industry and can be achieved through appropriate design (with replication and randomisation) and robust and appropriate modelling techniques.<p>Agronomic evaluation of plants – quantity and quality, fertilisers and herbicides and pest control strategies are achieved through agronomic experiments, usually grown in the field, glasshouse or temperature control rooms. During the past three years staff from SAGI-STH training program have developed and run a series of statistical workshops, using adult learning methodologies, to improve the statistical competency of agronomists who conduct these agronomic experiments.   The workshops start with an introduction to R, move through design and then analysis of agronomic experiments and end by introducing these scientists to reproducible research through R markdown. The workshops use active learning strategies and real-world examples relevant to the researchers.<p>On completion of the workshops, participants are then invited to join our community of practice, where we explore their data and work on solving statistical problems as a team. The community of practice is via online meetings, which are recorded so that participants can join synchronously or asynchronously. In this talk we explore the benefits of these workshops and community of practice and what it has meant in practice to the agronomists. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="4" data-target=".4collapsed">
            <td>Esther Meenken</td>
            <td>Uncertainty in digital agriculture: An interdisciplinary perspective</td>
            <td>9:50 - 10:10</td>
            <td></td>
          </tr>
          <tr class="out budgets 4collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Digitilisation in agricultural systems provides more, and increasingly real time, data to farmers, consumers, and others along the supply chain than ever before. Turning this data into actionable knowledge through eg bio-physical or statistical models should aid decisions related to activities such as farm management, policy development and product selection. A connected farm has data generating IoT systems proliferated throughout: In and on the soil, monitoring weather patterns and crop growth, and tracking the movement, interactions and welfare of animals. This burgeoning supply of data is accompanied by uncertainty that carries through and is modified as users perceive and interact with it.  For example, a researcher building a crop simulation model will be subject to sensor uncertainties due to bias, lack of precision, sensor failure and incomplete calibration of the sensors.  The model will exhibit direct uncertainties reflecting a) these measurement errors as well as b) lack of complete knowledge about the system being modelled and c) scenario specification. On the other hand, a farmer using information provided by the model to manage irrigation allocation may additionally experience indirect uncertainties stemming from, e.g., a lack of trust in the modellers or communicators which decrease the perceived certainty of a model, particularly in contexts where trust functions as a coping mechanism for an inability to fully assess a model’s direct uncertainty. Uncertainty may also arise due to ‘contextual uncertainty’, defined as the bio-physical or socio-cultural environment which shapes the structure of the model or the way it is used, and determines how ‘fit-for-purpose’ it may be. We hypothesise a conceptual framework that attempts to visually indicate the relative uncertainties as data and biophysical/sociocultural uncertainty are incorporated into a model and as the model in turn becomes incorporated into wider, and increasingly complex, contexts once shared and used. <br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
    <!-- Thurs -->
  </div>
  <div id="friday" class="tab-pane fade">
    <!-- Fri -->
    
    <!-- Fri -->
  </div>
</div>
