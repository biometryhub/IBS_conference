+++
title = "Abstracts and Titles"
description = ""
+++



<ul class="nav nav-tabs">
  <li class="active"><a data-toggle="tab" href="#tuesday">Tue</a></li>
  <li><a data-toggle="tab" href="#wednesday">Wed</a></li>
  <li><a data-toggle="tab" href="#thursday">Thu</a></li>
  <li><a data-toggle="tab" href="#friday">Fri</a></li>
</ul>
<div class="tab-content">
  <div id="tuesday" class="tab-pane fade in active">
    <h2>Invited Speakers: Medical</h2>
    <h4><i>Room: Exhibition Hall</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse1" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive">
      <table class="abstract-table">
        <thead>
          <tr>
            <th>Presenter</th>
            <th>Abstract Title</th>
            <th>Time</th>
            <th>Slides</th>
          </tr>
        </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="9" data-target=".9collapsed">
            <td>James Carpenter</td>
            <td>Multilevel multiple imputation for health and survey data: your flexible (and robust) friend
            </td>
            <td>9:30 - 10:30</td>
            <td></td>
          </tr>
          <tr class="out budgets 9collapsed collapse multi-collapse1" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Multiple imputation is now well established as a practical and flexible method for analyzing partially observed data under the missing at random assumption. However, in large datasets there are concerns about how to preserve heterogeneity in the relationship between variables in the imputation process.<p>Building on recent work, we describe an imputation model (and R software) which allows the covariance matrix of the variables to vary randomly across higher level units, which may represent health districts or hospitals.<p>We further show how this approach allows us to (i) include weights, when the substantive model is weighted; (ii) provide a degree of robustness to misspecification of the imputation model and (iii) extends to impute data consistent with interaction and non-linear effects under investigation.<p>We illustrate with examples from the UK Millennium Cohort Study and the UK Clinical Practice Research Datalink.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="10" data-target=".10collapsed">
            <td>Max Moldovan</td>
            <td>Pursuing the cancer-schizophrenia disassociation paradox: genomes, phenomes and intimate conversatio</td>
            <td>13:30 - 14:30</td>
            <td></td>
          </tr>
          <tr class="out budgets 10collapsed collapse multi-collapse1" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Positive and negative empirical findings of schizophrenia being protective against cancer remain a controversy and an active topic for debates among the intersection of oncologists, psychiatrists, epidemiologists and a diverse group of research scientists interested in the topic. I will present the vision of the paradox from different points of view, using a number of analysis and visualisation approaches from my current data science toolbox.<p>Firstly, I will take a position of a "genes-rule-it-all" theory proponent (while, in fact, being an opponent of the paradigm). I will share my experience of the exposure to genomics, and how my hopes and excitement were cut short by the inability of a SNP model to predict a well-defined phenotype when taken out of sample. Staying in the same role, I will introduce the Molecular Signatures Database (MSigDB) and an attempt to look at this gene expression signature information resource from a different angle.<p>Secondly, I will review epidemiological explanations behind the cancer-schizophrenia disassociation paradox, presenting the pair of disorders within the whole phenome network. While biases can help to justify the paradox, the doubt remains when one looks at bare numbers.<p>Finally, I will introduce a "geocentrism versus heliocentrism in cancer research" hypothesis and speculate about the role of dogmas in science, rates of clinical translation and prospects of moving to the brighter future.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="11" data-target=".11collapsed">
            <td>Richard Cook</td>
            <td>Missing outcomes in stepped-wedge trials</td>
            <td>11:40 - 12:00</td>
            <td></td>
          </tr>
          <tr class="out budgets 11collapsed collapse multi-collapse1" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          TBA<br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
    <!--                -->
    <!----First Stream --->
    <!--                -->
    <h2>Contributed Talks Session 1a: Biostatistics I</h2>
    <h4><i>Room: The Gallery</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive tg-wrap">
      <table class="abstract-table">
        <thead>
          <tr>
            <th>Presenter</th>
            <th>Abstract Title</th>
            <th>Time</th>
            <th>Slides</th>
          </tr>
        </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="1" data-target=".1collapsed">
            <td>Hans Hockey</td>
            <td>Hockey sticks and broken sticks continued – enhancing the gold standard RCT for chronic diseases</td>
            <td>11:00 - 11:20</td>
            <td></td>
          </tr>
          <tr class="out budgets 1collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          This work was motivated by a previously given chronic rare disease real data example as analysed by a longitudinal hockey stick model. The gold standard randomized controlled trial (RCT) compares active and placebo treatment arms at a post-baseline timepoint sufficiently late enough for an active effect to be apparent. For this design the analysis of covariance of final values adjusted for baseline values is standard (hopefully!).<p>Similar but enhanced alternative design and analysis combinations in chronic diseases will be presented which have several possible advantages: there is the possibility of assessing the size of any placebo effect; there is less ethical and recruitment need to have the active group larger than the placebo group; missing data, particularly of the final value, is less critical to the analysis.<p>It is also conjectured that some of the circumstances which can favour these enhanced designs can also be used to help argue for single arm studies in rare chronic diseases, despite the lack of randomization.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="2" data-target=".2collapsed">
            <td>Jessica Kasza</td>
            <td>From the stepped wedge to the staircase: the information content of stepped wedge trials</td>
            <td>11:20 - 11:40</td>
            <td></td>
          </tr>
          <tr class="out budgets 2collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Stepped wedge cluster randomised trials are a type of longitudinal cluster randomised trial in which clusters, e.g. schools, hospitals, or geographical regions, are randomised to a particular set of treatment sequences. In standard stepped wedge trials, all clusters start in the control condition before switching, in a randomised order, to the intervention. The application of stepped wedge trials is increasing rapidly: a 2011 systematic review of published or registered stepped wedge trials found only 25, but as of July 2019, 210 stepped wedge trials were registered on clinicaltrials.gov. Stepped wedge trials are particularly useful in assessing interventions that will be rolled out or cannot be undone, e.g. changes in policies or cluster-wide education campaigns. However, stepped wedge trials are expensive and burdensome, requiring that all clusters contribute measurements for the entire trial duration. Recent work has shown that different cluster-period “cells” of the stepped wedge contribute different amounts of information to the estimation of the intervention effect. Such work suggests that “incomplete” stepped wedge trials in which clusters contribute measurements in a restricted set of trial periods may provide efficient alternatives to the full stepped wedge. <p>In this talk I will discuss the amount of information contributed by cluster-period cells of stepped wedge trials to the estimation of the effect of an intervention, where this is quantified by the increase in the variance of the intervention effect estimator when that cell is omitted. I will consider the impact of within-cluster correlation structure, treatment effect heterogeneity, implementation periods, and unequal cluster-period sizes on the pattern of information content. This work indicates that in many scenarios, “staircase” trials, a particular type of incomplete stepped wedge in which clusters provide measurements immediately before and after the treatment switch only, may prove to be efficient and less burdensome alternatives to the complete stepped wedge.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="3" data-target=".3collapsed">
            <td>Md Anower Hossain</td>
            <td>Missing outcomes in stepped-wedge trials</td>
            <td>11:40 - 12:00</td>
            <td></td>
          </tr>
          <tr class="out budgets 3collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          In stepped-wedge cluster randomised trials (SW-CRTs), clusters are sequentially randomised until the point at which all clusters are exposed to the intervention. Two commonly used analysis approaches  are cluster-level analysis and individual-level analysis using linear mixed models (LMM). Missing outcomes are a commonly  occurring problem in SW-CRTs which can lead to invalid and misleading inferences if ignored or handled inappropriately. In SW-CRTs, it is plausible to have missing outcomes  depending on baseline covariates. In this paper, we considered only continuous outcomes and missingness only in outcomes depending on baseline covariates. We investigate analytically and by simulations the validity of cluster level analysis and LMM using complete record analysis (CRA) and multiple imputed data sets. Cluster level analysis using CRA gives a biased estimate unless the missingness mechanism is the same between the two intervention groups. LMM using CRA gives valid estimate regardless of the missingness mechanisms is the same or different between the intervention groups. On the basis of the simulation study and analytical results, we give guidance on the conditions under which each approach is valid. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="4" data-target=".4collapsed">
            <td>Michael T Fahey</td>
            <td>Longitudinal analysis of heterogeneity in rate of change in PSA level among men with low-grade prost</td>
            <td>12:00 - 12:20</td>
            <td></td>
          </tr>
          <tr class="out budgets 4collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Men diagnosed with low-grade prostate cancer (CAP) may be offered active surveillance (AS) instead of active treatment, which can have adverse side effects on quality of life.  Men on AS are monitored with a regimen of follow-up tests.  Low-grade CAP typically has a long natural history and rapid progression to advanced disease is rare.  However, about 30% of men discontinue AS within five years to have active treatment.  The prostate specific antigen (PSA) test measures the level of a protein in blood and indicates activity in the prostate.  It is not invasive and is inexpensive.<p>This study aims to: 1) estimate the rate of change in PSA level among men on AS, and 2) investigate whether there is heterogeneity in PSA trajectories.<p>279 men in an AS registry in Victoria were studied.  Mean follow-up was 3.5 years and PSA results were available for a median of 11 occasions per man (IQR: 6-18).<p>Longitudinal analysis involved regression of PSA level on time since diagnosis and adjusted for age at diagnosis.  A structural equations modelling approach was used to allow random variation in initial PSA level and in the rate of change of PSA level.  Latent classes in the random coefficients were estimated to investigate heterogeneity.<p>For most men, PSA level increased slowly and the rate of change was approximately 1.5 ng/ml per 5 years.  However, there was evidence that among a small subset of men (18%), the rate of change was five times greater.<p>The existance of a sub-group of men on AS with rapidly increasing PSA levels could indicate the need for a more frequent surveillance regimen and/or for targeted treatment.  Future work needs to focus on validation of this sub-group.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="5" data-target=".5collapsed">
            <td>Graham Hepworth</td>
            <td>Estimation of proportions by group testing with retesting of positive groups</td>
            <td>12:20 - 12:40</td>
            <td></td>
          </tr>
          <tr class="out budgets 5collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          In group testing (or pooled testing), material from individuals is pooled and tested in aggregate for the presence of an attribute, usually a disease.  Group testing for estimation of a proportion p has been applied in a wide range of fields, including virus prevalence in flowers, blood testing (especially for HIV), and transmission of viruses by mosquitoes.  Improved precision usually requires the testing of more groups, but in some situations it is difficult or expensive to obtain the required additional individuals.  If the testing procedure is non-destructive, retesting of groups comprising different combinations of individuals may be a useful option. <p>Hepworth & Watson (2017) developed an estimator of p for the retesting of a random grouping of individuals from the positive groups at the first stage.  The analytic complexity of this estimator led them to use simulation to examine its variance properties.  We have developed two closed-form analytic expressions for the variance of the second-stage estimator, and compared its performance with the results from the simulation.<p>Our analytical solutions give acceptable approximations in a reasonable range of circumstances.  They are most acceptable when the number of groups is not small and p is not large.  This is a useful result for group testing, which to be of major benefit, relies on a reasonable number of groups and a small to moderate prevalence.<p>Hepworth G & Watson RK (2017) Revisiting retesting in the estimation of proportions by group testing.  Communications in Statistics – Simulation and Computation 46:261–274.<p>Hepworth G & Walter SD (2019) Estimation of proportions by group testing with retesting of positive groups.  Communications in Statistics – Theory and Methods DOI:10.1080/03610926.2019.1620280). <br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
    <!--                 -->
    <!----Second Stream --->
    <!--                 -->
    <h2>Contributed Talks Session 1b: Modelling the Environment</h2>
    <h4><i>Room: Exhibition Hall</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive tg-wrap">
      <table class="abstract-table">
        <thead>
          <tr>
            <th>Presenter</th>
            <th>Abstract Title</th>
            <th>Time</th>
            <th>Slides</th>
          </tr>
        </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="1" data-target=".1collapsed">
            <td>Rune Christiansen</td>
            <td>Towards causal inference for spatio-temporal data: adjusting for time-invariant latent confounders</td>
            <td>11:00 - 11:20</td>
            <td></td>
          </tr>
          <tr class="out budgets 1collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          In statistical causality, we are interested not only in modeling the behaviour of a system that is passively observed, but also how the system reacts to changes in the data generating mechanism. Given knowledge of the underlying causal structure, such interventional behaviour can often be estimated from purely observational data (e.g., using covariate adjustment). Typically, the assumption is that data are generated as independent replications from the same underlying mechanism — an assertion that is often hard to justify in practice: geographically varying conditions in which the system is embedded induce spatial heterogeneity, and close-by observations (in space and time) tend to be strongly dependent. In this talk, I present causal models for spatio-temporal data that are adapted to these characteristics, and introduce a simple approach that allows for the estimation of causal effects under the influence of arbitrarily many latent confounders, as long as these confounders do not vary across time. Non-parametric hypothesis tests for the existence of causal effects are constructed based on data resampling, and do not rely on any distributional assumptions on the spatial dependence structure of the data. The method is applied to the problem of inferring the (potential) causal relationship between armed conflict and tropical forest loss, based on a spatio-temporal data set from Colombia. This talk does not require any prior knowledge in causal inference. <br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="2" data-target=".2collapsed">
            <td>Francis Hui</td>
            <td>Spatial Confounding in GEEs – Why the Working Correlation Matters (well, sort of)</td>
            <td>11:20 - 11:40</td>
            <td></td>
          </tr>
          <tr class="out budgets 2collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Generalized Estimating Equations (GEEs) are a popular tool in many scientific disciplines for investigating the effects of covariates on the mean of a response. In the context of spatial data analysis, GEEs rely on specifying a regression model for the marginal mean, a variance function, and a spatial working correlation matrix characterizing the spatial autocorrelation between observational units. One of the main advantages of GEEs is that estimation of the covariate effects is robust to misspecification of the choice of (spatial) working correlation matrix: the choice only affects the efficiency of the estimator. <p>In ongoing research, we investigate the impact of spatial confounding in GEEs. That is, what happens when the covariates included in a GEE, where a spatial working correlation matrix is used, are also spatially correlated. Under the conditional mixed model approach, the issue of spatial confounding is explicit and arises due to artificial multicollinearity between the spatially correlated covariates and the spatial random effect. We show that for GEEs, such multicollinearity also arises but occurs implicitly between spatially correlated covariates and the spatial working correlation matrix. Results suggests different choices of the working correlation matrix can lead to different attributions of the effect of the covariate on the mean versus on the spatial correlation i.e., on the first versus second moment. In turn, we consider using a so-called “restricted spatial working correlation matrix” that ensures all the variability in the direction of the covariates is attributed to the marginal mean, and is more in line with the underlying aim of GEEs. The issue of standard error estimation via the sandwich covariance matrix, and how it is impacted by spatial confounding, will also be discussed.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="3" data-target=".3collapsed">
            <td>Emy Guilbault</td>
            <td>Fitting species distribution models with uncertain species labels using point process models.</td>
            <td>11:40 - 12:00</td>
            <td></td>
          </tr>
          <tr class="out budgets 3collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          The popularity of species distribution models (SDMs) and the suite of tools to fit them have greatly increased over the last decade. The most common type of species data is presence-only data, which comes from citizen science. Point process models (PPMs) provide a flexible way to fit SDMs to presence-only data. Nonetheless, the quality of presence-only records (both identification and positional location) that serve as input to presence-only SDMs can be questioned. However most species distribution modelling methods applied to presence-only data assume certainty about species identity, but there are many practical situations in which this may not be the case. As an example, observers can be unable to clearly differentiate close species and taxonomists can split a species into multiple distinct species. We investigate the latter case, in which the species identities of records prior to a taxonomic change are confounded. In this talk, I will present two new tools for accommodating confounded records in PPMs. With these tools, we reclassify and incorporate records with uncertain species identities via finite mixture modelling or an iterative algorithm inspired by machine learning methods. Through simulation, we compare performance in classification and in prediction of these tools with different implementations to a standard approach which uses only records with known species labels, varying species abundance, correlation among species distributions, and the proportion of records with missing species labels. We also apply the best-performing methods to fit the distribution of 3 species of native Australian frogs that belong to the genus Myxophies. Among these species, 2 were newly described in 2006 in the northern range of the genus distribution and thus confounded previous records in the area made over the previous few decades.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="4" data-target=".4collapsed">
            <td>Ian Renner</td>
            <td>Modelling species communities using presence-only data</td>
            <td>12:00 - 12:20</td>
            <td></td>
          </tr>
          <tr class="out budgets 4collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Often, the only available data to serve as input for species distribution models is presence-only data, which consists of observation records for the target species with no corresponding absence information. While a number of presence-only species distribution modelling methods can relate the occurrence patterns to the environment, there are limits to the types of inference that can be made from presence-only data. Indeed, there are a number of biotic and abiotic factors that influence the distribution of species and the composition of species communities, such as species co-occurrence patterns, traits, and phylogenetic relationships among species, and it is presently difficult to incorporate these factors into presence-only models. <p>With other data inputs such as presence-absence and count data, such inference about species and species communities is available via the hierarchical modelling of species communities (HMSC) platform. With HMSC, users can partition the variation in species occurrences to components that relate to environmental filtering, species interactions, and random processes, allowing for both species-level and community-level inference. However, the HMSC package does not currently support presence-only input.<p>In this talk, I will present developments of presence-only input into the HMSC framework and demonstrate the new types of inference available as a result. These developments not only enable such inference for presence-only data, but likewise allow previous analyses of species communities from HMSC to be enriched by incorporating relevant presence-only data.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="5" data-target=".5collapsed">
            <td>Louise McMillan</td>
            <td>TBC</td>
            <td>12:20 - 12:40</td>
            <td></td>
          </tr>
          <tr class="out budgets 5collapsed collapse multi-collapse" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          TBC<br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
    <!--                -->
    <!----First Stream --->
    <!--                -->
  <h2>Contributed Talks Session 2a: Imputation</h2>
    <h4><i>Room: The Gallery</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse2" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive  tg-wrap">
      <table class="abstract-table">
        <thead>
          <tr>
            <th>Presenter</th>
            <th>Abstract Title</th>
            <th>Time</th>
            <th>Slides</th>
          </tr>
        </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="6" data-target=".6collapsed">
            <td>Nidhi Menon</td>
            <td>The effect of number of clusters and cluster sizes on multiple imputation in multilevel models</td>
            <td>16:00 - 16:20</td>
            <td></td>
          </tr>
          <tr class="out budgets 6collapsed collapse multi-collapse2" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Missing data are a common phenomenon in public health research. Multiple Imputation (MI) has been long recognized as an attractive approach to handle missing values. Statisticians are now advocating the use of MI as a gold standard in solving the missing data problem. Despite its early conception and its numerous advantages over the traditional ad hoc methods, there is still limited application of MI in public health research. <p>The theory of multiple imputation requires that imputations be made conditional on the sampling design. Not accounting for complex sample design features, such as stratification and clustering, during imputations can yield biased estimates from a design-based perspective. <p>Most datasets in public health research show some form of natural clustering (individuals within households, households within the same district, patients within wards, etc.).  Cluster effects are often of interest in health research. In this study, we investigate through simulations different strategies for accounting for clustering when multiply imputing variables. Recent studies have identified methods to include fixed effects for clusters in imputations, however there is limited information on impact of varying number of clusters and cluster sizes on MI. <p>In this study, we simulate 3 level hierarchical data structures varying the number of clusters at each level. Missing values are present in covariates at each level in the data. We consider the impact of the combination of varying cluster sizes and proportion of missingness in imputation of covariates at each level in the dataset. This study implements the Gelman and Hill approach for imputation of missing data at higher levels by including aggregate forms of individual level measurements to impute for missing values at higher levels. The performance of popular methods of imputations, MICE and JoMo are compared. Performance measures include bias in estimates, mean squared errors and probability coverage of confidence intervals<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="7" data-target=".7collapsed">
            <td>Thomas Sullivan</td>
            <td>Multiple imputation for missing outcome data in trials involving independent and paired observations</td>
            <td>16:20 - 16:40</td>
            <td></td>
          </tr>
          <tr class="out budgets 7collapsed collapse multi-collapse2" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Background: Trials involving a mixture of independent and paired data arise in many areas of health research, for example in paediatrics where outcomes can be collected on singletons and twins, and in ophthalmology, where one or both eyes may require treatment. An important consideration in these trials is the correlation in outcomes, or clustering, that occurs between observations from the same pair. When applying multiple imputation (MI) to address missing data, previous research suggests that any clustering in the data should be accounted for in the imputation and analysis models. However, most ad-hoc methods of MI for clustered data were designed with large and/or equal sized clusters in mind, and it is unclear how MI should be imeplemented in settings with independent and paired data.<p>Methods: Using simulated data the following MI approaches were evaluated: (1) MI ignoring clustering; (2) MI using chained equations with conditional imputation of the 2nd member of a pair; (3) MI performed separately by cluster size, and; (4) multi-level MI. Observations were allocated to one of two treatment groups using simple randomisation, with members of a pair randomised individually, to the same (cluster randomisation) or to opposite groups (opposite randomisation). <p>Results: When outcome data were missing at random, all MI methods produced unbiased treatment effect estimates. Although performance deficits were small, MI ignoring clustering and chained equations with conditional imputation produced confidence intervals for the treatment effect that were too narrow under cluster randomisation and too wide under opposite randomisation. MI performed separately by cluster size and multi-level MI performed well across the range of scenarios considered.<p>Conclusions: In trials involving a mixture of independent and paired observations, particularly those employing cluster or opposite randomisation, we recommend researchers apply multi-level MI or standard MI performed separately by cluster size to address missing outcome data.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="8" data-target=".8collapsed">
            <td>Patrick Graham</td>
            <td>Adjusting for linkage bias in the analysis of record linked data.</td>
            <td>16:40 - 17:00</td>
            <td></td>
          </tr>
          <tr class="out budgets 8collapsed collapse multi-collapse2" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Data formed the application of record linkage techniques to two or more datasets are increasingly important in public health and social science research. Regardless of the linkage method, it is commonly the case that not all records can be linked. If linkage rates vary by variables relevant to an analysis then analyses restricted to only linked records may be biased. Records in a base dataset that are not linked to a record in target dataset will have missing values for all variables recorded only on the target dataset. This produces a block missing-ness structure similar to that encountered in panel studies when study members decline participation in one or more study waves.  While missing data theory and methods are clearly applicable to this problem it is insightful to work through the specifics of missing data methodology as it applies to linkage bias. Both Bayesian and frequentist approaches to the problem are considered.  In certain circumstances complete-case analyse can be justified, though this depends on the role the partially observed variables play in analysis (e.g. outcome or exposure).  When complete-case analyses are not justified, the available analysis methods can be classified into those that condition on both linked and unlinked records and methods based only on linked cases with some adjustment for incomplete linkage.  The Bayesian approach leads naturally to the former perspective and leads to Gibbs sampling and multiple imputation as reasonable methods. Analytic approaches based on adjusted complete-case analyses fit best within a frequentist framework, and conditional likelihood and inverse probability weighting methods appear reasonable options.  A simulation study   confirms that multiple imputation, conditional likelihood and inverse probability weighting methods all adjust appropriately for linkage bias and achieve nominal interval coverage rates. Multiple imputation is generally more efficient that conditional likelihood or inverse probability weighting methods. <br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
    <!--                 -->
    <!----Second Stream --->
    <!--                 -->
    <h2>Contributed Talks Session 2b: Experimental Design</h2>
    <h4><i>Room: Exhibition Hall</i></h4>
    <a class="btn btn-template-main btn-sm" type="button" data-toggle="collapse" data-target=".multi-collapse2" aria-expanded="false" aria-controls="1collapsed 2collapsed 3collapsed">Expand All</a>
    <div class="table-responsive  tg-wrap">
      <table class="abstract-table">
      <thead>
        <tr>
          <th>Presenter</th>
          <th>Abstract Title</th>
          <th>Time</th>
          <th>Slides</th>
        </tr>
      </thead>
        <tbody>
          <tr class="clickable" data-toggle="collapse" id="6" data-target=".6collapsed">
            <td>Nidhi Menon</td>
            <td>The effect of number of clusters and cluster sizes on multiple imputation in multilevel models</td>
            <td>16:00 - 16:20</td>
            <td></td>
          </tr>
          <tr class="out budgets 6collapsed collapse multi-collapse2" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Missing data are a common phenomenon in public health research. Multiple Imputation (MI) has been long recognized as an attractive approach to handle missing values. Statisticians are now advocating the use of MI as a gold standard in solving the missing data problem. Despite its early conception and its numerous advantages over the traditional ad hoc methods, there is still limited application of MI in public health research. <p>The theory of multiple imputation requires that imputations be made conditional on the sampling design. Not accounting for complex sample design features, such as stratification and clustering, during imputations can yield biased estimates from a design-based perspective. <p>Most datasets in public health research show some form of natural clustering (individuals within households, households within the same district, patients within wards, etc.).  Cluster effects are often of interest in health research. In this study, we investigate through simulations different strategies for accounting for clustering when multiply imputing variables. Recent studies have identified methods to include fixed effects for clusters in imputations, however there is limited information on impact of varying number of clusters and cluster sizes on MI. <p>In this study, we simulate 3 level hierarchical data structures varying the number of clusters at each level. Missing values are present in covariates at each level in the data. We consider the impact of the combination of varying cluster sizes and proportion of missingness in imputation of covariates at each level in the dataset. This study implements the Gelman and Hill approach for imputation of missing data at higher levels by including aggregate forms of individual level measurements to impute for missing values at higher levels. The performance of popular methods of imputations, MICE and JoMo are compared. Performance measures include bias in estimates, mean squared errors and probability coverage of confidence intervals<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="7" data-target=".7collapsed">
            <td>Thomas Sullivan</td>
            <td>Multiple imputation for missing outcome data in trials involving independent and paired observations</td>
            <td>16:20 - 16:40</td>
            <td></td>
          </tr>
          <tr class="out budgets 7collapsed collapse multi-collapse2" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Background: Trials involving a mixture of independent and paired data arise in many areas of health research, for example in paediatrics where outcomes can be collected on singletons and twins, and in ophthalmology, where one or both eyes may require treatment. An important consideration in these trials is the correlation in outcomes, or clustering, that occurs between observations from the same pair. When applying multiple imputation (MI) to address missing data, previous research suggests that any clustering in the data should be accounted for in the imputation and analysis models. However, most ad-hoc methods of MI for clustered data were designed with large and/or equal sized clusters in mind, and it is unclear how MI should be imeplemented in settings with independent and paired data.
          Methods: Using simulated data the following MI approaches were evaluated: (1) MI ignoring clustering; (2) MI using chained equations with conditional imputation of the 2nd member of a pair; (3) MI performed separately by cluster size, and; (4) multi-level MI. Observations were allocated to one of two treatment groups using simple randomisation, with members of a pair randomised individually, to the same (cluster randomisation) or to opposite groups (opposite randomisation). 
          Results: When outcome data were missing at random, all MI methods produced unbiased treatment effect estimates. Although performance deficits were small, MI ignoring clustering and chained equations with conditional imputation produced confidence intervals for the treatment effect that were too narrow under cluster randomisation and too wide under opposite randomisation. MI performed separately by cluster size and multi-level MI performed well across the range of scenarios considered.
          Conclusions: In trials involving a mixture of independent and paired observations, particularly those employing cluster or opposite randomisation, we recommend researchers apply multi-level MI or standard MI performed separately by cluster size to address missing outcome data.<br><br>
          </td>
          </tr>
          <tr class="clickable" data-toggle="collapse" id="8" data-target=".8collapsed">
            <td>Patrick Graham</td>
            <td>Adjusting for linkage bias in the analysis of record linked data.</td>
            <td>16:40 - 17:00</td>
            <td></td>
          </tr>
          <tr class="out budgets 8collapsed collapse multi-collapse2" aria-expanded="false" style="height: 0px;">
          <td colspan="4">
          Data formed the application of record linkage techniques to two or more datasets are increasingly important in public health and social science research. Regardless of the linkage method, it is commonly the case that not all records can be linked. If linkage rates vary by variables relevant to an analysis then analyses restricted to only linked records may be biased. Records in a base dataset that are not linked to a record in target dataset will have missing values for all variables recorded only on the target dataset. This produces a block missing-ness structure similar to that encountered in panel studies when study members decline participation in one or more study waves.  While missing data theory and methods are clearly applicable to this problem it is insightful to work through the specifics of missing data methodology as it applies to linkage bias. Both Bayesian and frequentist approaches to the problem are considered.  In certain circumstances complete-case analyse can be justified, though this depends on the role the partially observed variables play in analysis (e.g. outcome or exposure).  When complete-case analyses are not justified, the available analysis methods can be classified into those that condition on both linked and unlinked records and methods based only on linked cases with some adjustment for incomplete linkage.  The Bayesian approach leads naturally to the former perspective and leads to Gibbs sampling and multiple imputation as reasonable methods. Analytic approaches based on adjusted complete-case analyses fit best within a frequentist framework, and conditional likelihood and inverse probability weighting methods appear reasonable options.  A simulation study   confirms that multiple imputation, conditional likelihood and inverse probability weighting methods all adjust appropriately for linkage bias and achieve nominal interval coverage rates. Multiple imputation is generally more efficient that conditional likelihood or inverse probability weighting methods. <br><br>
          </td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
  <div id="wednesday" class="tab-pane fade in active">
    <div class="table-responsive">
    <!-- Wed -->
    </div>
  </div>
  <div id="thursday" class="tab-pane fade in active">
    <div class="table-responsive">
    <!-- Thurs -->
    </div>
  </div>
  <div id="friday" class="tab-pane fade in active">
    <div class="table-responsive">
    <!-- Fri -->
    </div>
  </div>
</div>
